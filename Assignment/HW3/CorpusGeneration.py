from bs4 import BeautifulSoup
import string
import re
from urllib.request import urlopen as uReq
from urllib.request import urlretrieve

# The following code is used to generate a corpus using URLs crawled in the first assignment.
# It generates 1000 text files corresponding to 1000 URLs and the text files are numbered from
# 1 to 1000 to avoid duplicate file names.


# Corpus is generated by accessing the text file which holds 1000 URLs
def corpus_generation():
    lnks = open('HW1_Output.txt', 'r')
    lines = lnks.read().splitlines()
    counter = 1
    mapping = {}
    for line in lines:
        line = line
        split = line.split('/wiki/')
        value = split[1]
        # Writing the number of text files generated and its corresponding
        # URL name to a text file.
        mapping = {counter: value}
        print(mapping)
        with open("KeyValueMapping.txt", 'a') as out:
            out.write(str(mapping))
            out.write("\n")
            out.close
        # Function call to handle images, tables, references, etc.
        corpus_preprocess(line, counter)
        counter += 1


# This function handles tables, images, references, etc. present with in the document.
def corpus_preprocess(url, name):
    html_text = uReq(url)
    soup = BeautifulSoup(html_text, 'html.parser')

    for jscript in soup(["script", "style", "semantics", "dl"]):
        jscript.extract()

    title = soup.find('title').get_text()

    tkn = soup.findAll('a')
    b = ''
    for a in tkn:
        b = b + " " + a.get_text()

    tkn = soup.find("div", {"id": "mw-content-text"}).get_text()
    section = soup.find("span", {"class": "mw-editsection"} )
    if section:
        tkn = tkn.replace( section.get_text(), ' ' )

    tbl = soup.find( "table", {"class": "wikitable"} )
    if tbl:
        tkn = tkn.replace( tbl.get_text(), ' ' )

    image = soup.find("div", {"class": "thumb tright"})
    if image:
        tkn = tkn.replace(image.get_text(), ' ')

    reference = soup.find( "div", {"class": "reflist columns references-column-width"} )
    if reference:
        tkn = tkn.replace( reference.get_text(), ' ' )

    cnts = soup.find("div", {"id": "toc"})
    if cnts:
        tkn = tkn.replace(cnts.get_text(), ' ')

    tkns = title + tkn

    lns = (line.strip() for line in tkns.splitlines())

    pieces = (phrase.strip() for line in lns for phrase in line.split("  "))

    tkns = '\n'.join(piece for piece in pieces if piece)

    # Function call for tokenization
    preprocess = tokenize(tkns)

    file = str(name) + ".txt"
    with open(file,'w') as out:
        out.write(str(preprocess))
        out.close


# This function performs the operation of tokenization
def tokenize(term):
    term = re.sub('\n+', " ", term)
    term = re.sub(' +', " ", term)
    term = re.sub(r"(?<!\d)[.,;:](?!\d)", " ", term, 0)
    term = ' '.join(term.strip(string.punctuation) for term in term.split())
    term = re.sub('"', " ", term)
    term = re.sub('/', " ", term)
    term = re.sub(r"[()]", " ", term, 0)
    term = re.sub(r"[$]", " ", term, 0)
    term = re.sub(r'\[.*?\]', " ", term)
    term = re.sub('=', " ", term)
    term = bytes(term, "UTF-8")
    term = term.decode("ascii", "ignore")

    return term.lower()


corpus_generation()